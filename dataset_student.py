# -*- coding: utf-8 -*-
"""Dataset_Student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k4wBfX1tTmfhzaSsiyNbmgyPu4nQsihG
"""

from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score
import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV,KFold
from sklearn.svm import SVR,SVC
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score, classification_report,confusion_matrix,roc_curve,auc,mean_squared_error
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.neural_network import MLPClassifier,MLPRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LinearRegression, LogisticRegression
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import tensorflow as tf
from google.colab import drive
drive.mount("/content/drive")

np.random.seed(0)

import warnings
warnings.filterwarnings("ignore")







Data=pd.read_excel('/content/drive/My Drive/Freelance/Dataset_Student/Dataset_student.xlsx')

for col in Data.columns.to_list():
    if Data[col].dtype==object:
       Data[col]=LabelEncoder().fit_transform(Data[col]).astype(float)





#Binary_Classification
#َA configuration

X=Data.drop(columns=['G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if Y[i]>=10:
       Label.append('Pass')
    else:
       Label.append('Fail')   
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))





#Binary_Classification
#َB configuration

X=Data.drop(columns=['G2','G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if Y[i]>=10:
       Label.append('Pass')
    else:
       Label.append('Fail')   
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#Binary_Classification
#َC configuration

X=Data.drop(columns=['G1','G2','G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if Y[i]>=10:
       Label.append('Pass')
    else:
       Label.append('Fail')   
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))





#5-Level_Classification
#َA configuration

X=Data.drop(columns=['G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if 20>=Y[i]>=16:
       Label.append('A')
    elif 16>Y[i]>=14:
       Label.append('B')
    elif 14>Y[i]>=12:
       Label.append('C')
    elif 12>Y[i]>=10:
       Label.append('D')
    elif 10>Y[i]>=0:
       Label.append('F')        
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))





#5-Level_Classification
#َB configuration

X=Data.drop(columns=['G2','G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if 20>=Y[i]>=16:
       Label.append('A')
    elif 16>Y[i]>=14:
       Label.append('B')
    elif 14>Y[i]>=12:
       Label.append('C')
    elif 12>Y[i]>=10:
       Label.append('D')
    elif 10>Y[i]>=0:
       Label.append('F')        
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))





#5-Level_Classification
#َC configuration

X=Data.drop(columns=['G1','G2','G3']).values
Y=Data['G3'].values

Label=[]
for i in range(len(Y)):
    if 20>=Y[i]>=16:
       Label.append('A')
    elif 16>Y[i]>=14:
       Label.append('B')
    elif 14>Y[i]>=12:
       Label.append('C')
    elif 12>Y[i]>=10:
       Label.append('D')
    elif 10>Y[i]>=0:
       Label.append('F')        
    
Label=LabelEncoder().fit_transform(Label).astype(int)

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVC(class_weight='balanced',probability=True,kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Label)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPClassifier(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)

grid.fit(X, Label)

NN=grid.best_estimator_


clfs={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(class_weight='balanced'),
      'Random Forest':RandomForestClassifier(class_weight='balanced',n_estimators=500)}

classifiers=list(clfs.keys())
for clf in classifiers:
    scores = cross_val_score(clfs[clf], X, Label, cv=10)
    print(f" {clf}: %0.2f PCC with a standard deviation of %0.2f" % (scores.mean(), scores.std()))





#Regression
#َA configuration

X=Data.drop(columns=['G3']).values
Y=Data['G3'].values

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVR(kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Y)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPRegressor(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)


grid.fit(X, Y)

NN=grid.best_estimator_


models={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(),
      'Random Forest':RandomForestRegressor(n_estimators=500)}

kf = KFold(n_splits=10)

regs=list(models.keys())
for reg in regs:
    result=[]
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = Y[train_index], Y[test_index]
        regressor=models[reg]
        regressor.fit(X_train, y_train)
        y_pred=regressor.predict(X_test)
        rmse=mean_squared_error(y_test, y_pred, squared=False)
        result.append(rmse)
    result=np.array(result)
    print(f" {reg}: %0.2f RMSE with a standard deviation of %0.2f" % (result.mean(), result.std()))





#Regression
#َB configuration

X=Data.drop(columns=['G2','G3']).values
Y=Data['G3'].values

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVR(kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Y)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPRegressor(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)


grid.fit(X, Y)

NN=grid.best_estimator_


models={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(),
      'Random Forest':RandomForestRegressor(n_estimators=500)}

kf = KFold(n_splits=10)

regs=list(models.keys())
for reg in regs:
    result=[]
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = Y[train_index], Y[test_index]
        regressor=models[reg]
        regressor.fit(X_train, y_train)
        y_pred=regressor.predict(X_test)
        rmse=mean_squared_error(y_test, y_pred, squared=False)
        result.append(rmse)
    result=np.array(result)
    print(f" {reg}: %0.2f RMSE with a standard deviation of %0.2f" % (result.mean(), result.std()))





#Regression
#َC configuration

X=Data.drop(columns=['G1','G2','G3']).values
Y=Data['G3'].values

param_grid = {'gamma':[2 ** float(-9),2 ** float(-7),2 ** float(-5),2 ** float(-3),2 ** float(-1)]}

SVM = SVR(kernel='rbf')
grid = GridSearchCV(SVM ,param_grid,cv=5)

grid.fit(X, Y)


SVM=grid.best_estimator_


param_grid = {'hidden_layer_sizes':[ 2, 4, 6, 8]}

NN = MLPRegressor(solver='lbfgs', max_iter=100)
grid = GridSearchCV(NN ,param_grid,cv=5)


grid.fit(X, Y)

NN=grid.best_estimator_


models={'NV':GaussianNB(),
      'NN': NN,
      'SVM':SVM,
      'Decision Tree':DecisionTreeClassifier(),
      'Random Forest':RandomForestRegressor(n_estimators=500)}

kf = KFold(n_splits=10)

regs=list(models.keys())
for reg in regs:
    result=[]
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = Y[train_index], Y[test_index]
        regressor=models[reg]
        regressor.fit(X_train, y_train)
        y_pred=regressor.predict(X_test)
        rmse=mean_squared_error(y_test, y_pred, squared=False)
        result.append(rmse)
    result=np.array(result)
    print(f" {reg}: %0.2f RMSE with a standard deviation of %0.2f" % (result.mean(), result.std()))



